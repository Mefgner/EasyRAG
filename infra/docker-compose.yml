services:
  qdrant:
    image: "qdrant/qdrant:latest"
    ports: ["6333:6333"]
    volumes: [qdrant_storage:/qdrant/storage]
    environment:
      - QDRANT__STORAGE__USE_MMAP=false

  llama:
    image: "ghcr.io/ggml-org/llama.cpp:server-cuda"
    command: >
      --model /models/Qwen3-4B-it.gguf
      --no-webui
      --n-gpu-layers 40
      --flash-attn
      --batch-size 1024
      --cache-type-k q8_0
      --cache-type-v q8_0
      --parallel 2
    ports: ["8080:8080"]
    volumes: ["./models:/models"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia

volumes:
  qdrant_storage:
    driver: local
